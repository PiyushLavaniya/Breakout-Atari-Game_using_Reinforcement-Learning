{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a33fb7e-b2da-4aca-8282-54e3df941b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_atari_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39a6f9c-33c8-48af-af7a-df18cd85b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "##In this Model, we are going to train an Atari Breakout game, and we will be stacking 4 environments together and \n",
    "##training them at the same time.\n",
    "##We stack them using 'VecFrameStack'. In this the previous Projects, we were not using a Vectorized environment.\n",
    "##But here we are using a Vectorized Environment. And we will be training using images. Because the environment is image-based.\n",
    "##So we also use 'CnnPolicy'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f594dab-0771-4cbf-8d3d-7662ecba6f11",
   "metadata": {},
   "source": [
    "## Making and Testing the Environent using gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab644fac-e3e7-4689-8c08-c1edf1a44245",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'Breakout-v4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "745f6b59-0666-4dc4-930f-3b7e0cd0d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name, render_mode = 'human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "63d9381e-2f8f-43a5-a1a8-442a052b88e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]], dtype=uint8),\n",
       " {'lives': 5, 'episode_frame_number': 0, 'frame_number': 0})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()  ##This is our initial Observation or State."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b6ce741d-34bc-452a-ab61-e8035f84bdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "36123689-66ec-4620-b396-6cc7504ffa1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space  ##'(210, 160, 3)' as you can see it contains an image. And it got values from 0 to 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09b606f2-7113-49e8-94cd-d9532939381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: 3.0\n",
      "Episode: 2 Score: 1.0\n",
      "Episode: 3 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 3\n",
    "for episode in range(1, episodes+1):\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    terminated = False\n",
    "    done = False\n",
    "\n",
    "    while not terminated:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "    print(f\"Episode: {episode} Score: {score}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4a7e871e-996d-4a61-83f4-1dd5781875d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568a6af6-0dd6-4805-8930-f1430ca99718",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25756e39-9c04-419b-ac42-907149c60659",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari_env('Breakout-v4', n_envs = 4, seed = 0)  ##We will be using 4 environements at the same time for training. So we specified the 'n_envs' to be 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d44ff577-0890-4895-aad7-ab1a3943b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VecFrameStack(env, n_stack = 4)  ##Here we are stacking the 4 environments. Also Note that the environment is Vectorized in this Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b213d87d-4186-4255-a6aa-594ecf6ea537",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = os.path.join('Training', 'logs')  ##Specifying the Path to save the Logs for Tensorboard later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90ae0ca5-bb76-4f0b-9cf2-2dbaea674d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = A2C('CnnPolicy', env, verbose = 1, tensorboard_log = logs)  ##We used the 'CnnPolicy' since we are dealing with Images here and a Convolutional Neural Network will be way faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c52c1ae1-b9d6-4b69-bf71-4812a1f5bfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\logs\\A2C_1\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 274      |\n",
      "|    ep_rew_mean        | 1.53     |\n",
      "| time/                 |          |\n",
      "|    fps                | 43       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 46       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.0609   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.0452  |\n",
      "|    value_loss         | 0.0779   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 279      |\n",
      "|    ep_rew_mean        | 1.61     |\n",
      "| time/                 |          |\n",
      "|    fps                | 58       |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 68       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -0.00938 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.105   |\n",
      "|    value_loss         | 0.0259   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 284      |\n",
      "|    ep_rew_mean        | 1.66     |\n",
      "| time/                 |          |\n",
      "|    fps                | 65       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 91       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.481    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.0162  |\n",
      "|    value_loss         | 0.0248   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 283      |\n",
      "|    ep_rew_mean        | 1.57     |\n",
      "| time/                 |          |\n",
      "|    fps                | 71       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 111      |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.659    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.0165   |\n",
      "|    value_loss         | 0.0377   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 291      |\n",
      "|    ep_rew_mean        | 1.71     |\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 130      |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.855    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.0702   |\n",
      "|    value_loss         | 0.0444   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x1d6ac3b2608>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps = 10000)  ##You will be training for Significantly longer for this agent to work better in this Environment. Around '1 to 2 Million' steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8a2914c-8cdb-4ba6-9e3b-41f1bf7c960a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\logs\\A2C_2\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 322      |\n",
      "|    ep_rew_mean        | 2.33     |\n",
      "| time/                 |          |\n",
      "|    fps                | 107      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.567   |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.0103  |\n",
      "|    value_loss         | 0.0281   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 313      |\n",
      "|    ep_rew_mean        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    fps                | 107      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 37       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.624   |\n",
      "|    explained_variance | 0.199    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.231   |\n",
      "|    value_loss         | 0.477    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 315      |\n",
      "|    ep_rew_mean        | 2.21     |\n",
      "| time/                 |          |\n",
      "|    fps                | 107      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 55       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.0216  |\n",
      "|    value_loss         | 0.0249   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 320       |\n",
      "|    ep_rew_mean        | 2.31      |\n",
      "| time/                 |           |\n",
      "|    fps                | 108       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 73        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.19     |\n",
      "|    explained_variance | 0.966     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.000757 |\n",
      "|    value_loss         | 0.0193    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 327      |\n",
      "|    ep_rew_mean        | 2.41     |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 92       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.279    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.316    |\n",
      "|    value_loss         | 0.29     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x1d6ac3b2608>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f159b-2d4c-47b8-9731-689cb5f5fa3b",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e14b5c10-006f-4a50-a629-547805c39187",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_save_path = os.path.join('Training', 'Saved_Models', 'A2C_Breakout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26ca17f3-a9e7-4377-b0bc-4fb7b14b33d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PIYUSH_LAVANIYA_CMD\\DEEP_LEARNING_Projects\\Reinforcement_Learning\\Rl\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:278: UserWarning: Path 'Training\\Saved_Models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "model.save(training_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6307d61e-859a-4a1d-86b8-0aba96e15479",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "394e3b5c-87a4-4d8d-8756-4d4a9694f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari_env('Breakout-v4', n_envs = 1, seed = 0)  ##Rememeber we used 4 environments to train but now we wanna test and evaluate our model on only one environment like we usually do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19738ba3-6bf8-494e-b5da-bb55371e79e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VecFrameStack(env, n_stack = 4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "55b2e440-f394-42c9-9ff1-d03bf88dc16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = A2C.load(training_save_path, env)  ##Loading the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13440ef9-8dfc-42ff-8883-b5043651aa5d",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea11c4be-2781-4399-ad7d-7183b5a3e8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.6666666666666665, 0.4714045207910317)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes = 3, render = True)  ##This is the average reward score and average episode length metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76551a69-bb92-43bd-b066-4abe9b3b755d",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b90e7995-5fc2-4c34-b1c8-a6f205db678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari_env('Breakout-v4', n_envs = 1, seed = 0)\n",
    "env = VecFrameStack(env, n_stack = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13f485-f55a-43c9-80ce-6ad63f5f0462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _  = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render(\"human\")  ##While using 'make_atari_env' method to create your environment, specify the render mode in the 'render()' method, to render your Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "222a9c53-c9f2-4859-84ff-e17d4aa599af",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "da3e2e50-7767-494a-b042-ae5a87678f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: [0.]\n",
      "Episode: 2 Score: [0.]\n",
      "Episode: 3 Score: [2.]\n",
      "Episode: 4 Score: [0.]\n",
      "Episode: 5 Score: [0.]\n",
      "Episode: 6 Score: [0.]\n",
      "Episode: 7 Score: [0.]\n",
      "Episode: 8 Score: [2.]\n",
      "Episode: 9 Score: [0.]\n",
      "Episode: 10 Score: [0.]\n"
     ]
    }
   ],
   "source": [
    "##We can also use something else.\n",
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    score = 0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        env.render(\"human\")\n",
    "        score += reward\n",
    "    print(f\"Episode: {episode} Score: {score}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "73de1d78-5cb1-4282-b7c7-a157c6121ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##As you can see this Model is not working well at all, the maximum score it is able to get was 2.\n",
    "##The Longer you train your Model for the better it will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb0412a-8ff2-4002-8bd7-d96166f87506",
   "metadata": {},
   "source": [
    "## Testing a larger Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "db629272-378f-4b39-a186-45246c033ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PIYUSH_LAVANIYA_CMD\\DEEP_LEARNING_Projects\\Reinforcement_Learning\\Rl\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:96: UserWarning: You loaded a model that was trained using OpenAI Gym. We strongly recommend transitioning to Gymnasium by saving that model again.\n",
      "  \"You loaded a model that was trained using OpenAI Gym. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PIYUSH_LAVANIYA_CMD\\DEEP_LEARNING_Projects\\Reinforcement_Learning\\Rl\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:753: UserWarning: You are probably loading a model saved with SB3 < 1.7.0, we deactivated exact_match so you can save the model again to avoid issues in the future (see https://github.com/DLR-RM/stable-baselines3/issues/1233 for more info). Original error: Error(s) in loading state_dict for ActorCriticCnnPolicy:\n",
      "\tMissing key(s) in state_dict: \"pi_features_extractor.cnn.0.weight\", \"pi_features_extractor.cnn.0.bias\", \"pi_features_extractor.cnn.2.weight\", \"pi_features_extractor.cnn.2.bias\", \"pi_features_extractor.cnn.4.weight\", \"pi_features_extractor.cnn.4.bias\", \"pi_features_extractor.linear.0.weight\", \"pi_features_extractor.linear.0.bias\", \"vf_features_extractor.cnn.0.weight\", \"vf_features_extractor.cnn.0.bias\", \"vf_features_extractor.cnn.2.weight\", \"vf_features_extractor.cnn.2.bias\", \"vf_features_extractor.cnn.4.weight\", \"vf_features_extractor.cnn.4.bias\", \"vf_features_extractor.linear.0.weight\", \"vf_features_extractor.linear.0.bias\".  \n",
      "Note: the model should still work fine, this only a warning.\n",
      "  \"You are probably loading a model saved with SB3 < 1.7.0, \"\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join('Training', 'Saved_Models', 'A2C_2M_model')\n",
    "model = A2C.load(model_path, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7f59acba-1595-443e-ac9e-94d1cb4e81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari_env('Breakout-v4', n_envs = 1, seed = 0)\n",
    "vec_env = VecFrameStack(env, n_stack = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d645f7e8-43e1-47b4-aca3-d2b1232b06c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PIYUSH_LAVANIYA_CMD\\DEEP_LEARNING_Projects\\Reinforcement_Learning\\Rl\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:365: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21.3, 9.132907532653553)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, vec_env, n_eval_episodes = 10, render = True)  ##Now as you can see the average reward value is 21.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c23719-3ef9-43e2-b632-9f27590c7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _  = model.predict(obs)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    env.render(\"human\")  ##While using 'make_atari_env' method to create your environment, specify the render mode in the 'render()' method, to render your Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7f9074f1-e9fc-41a6-b6fc-1ae2c68a02ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb02cd6a-383e-4e09-b7d9-a49fd79d746f",
   "metadata": {},
   "source": [
    "## To save the GIF of your Agent's Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "49ec422a-60c0-4edd-a9e0-431d2b86e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bddcea-bb2a-49bb-8b94-d43ebadb3d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "obs = vec_env.reset()\n",
    "img = vec_env.render(mode=\"rgb_array\")\n",
    "for i in range(350):\n",
    "    images.append(img)\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, _ ,_ = vec_env.step(action)\n",
    "    img = vec_env.render(mode=\"rgb_array\")\n",
    "\n",
    "imageio.mimsave(\"A2C_Breakout2.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], duration = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a246305-4c5d-4cb3-8b4d-6fcdbc954215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
